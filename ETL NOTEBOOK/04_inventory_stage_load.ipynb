{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a2cb0f3-f863-4fd3-adfa-3957db9608bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Inventory Data Stage Load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aea6fd08-35f9-4c86-aee1-b9b0f2055776",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**This notebook processes inventory data from source files and loads it into the staging table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "238caa91-b478-4152-a399-bc42a74b20ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing inventory data from: /Volumes/event-driven-catalog/default/incremental_load/inventory_data/source/\nStaging table: `event-driven-catalog`.default.inventory_stage\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "source_dir = \"/Volumes/event-driven-catalog/default/incremental_load/inventory_data/source/\"\n",
    "archive_dir = \"/Volumes/event-driven-catalog/default/incremental_load/inventory_data/archive/\"\n",
    "stage_table = \"`event-driven-catalog`.default.inventory_stage\"\n",
    "\n",
    "\n",
    "print(f\"Processing inventory data from: {source_dir}\")\n",
    "print(f\"Staging table: {stage_table}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbf03fe9-4db4-412f-bb23-4e067cd1798a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema defined for inventory data\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Define schema for inventory data\n",
    "inventory_schema = StructType([\n",
    "    StructField(\"inventory_id\", StringType(), False),\n",
    "    StructField(\"product_id\", StringType(), False),\n",
    "    StructField(\"warehouse_id\", StringType(), False),\n",
    "    StructField(\"warehouse_name\", StringType(), False),\n",
    "    StructField(\"location\", StringType(), False),\n",
    "    StructField(\"stock_quantity\", IntegerType(), False),\n",
    "    StructField(\"reserved_quantity\", IntegerType(), False),\n",
    "    StructField(\"available_quantity\", IntegerType(), False),\n",
    "    StructField(\"reorder_level\", IntegerType(), False),\n",
    "    StructField(\"last_restocked\", DateType(), False),\n",
    "    StructField(\"last_audit\", DateType(), False),\n",
    "    StructField(\"created_timestamp\", TimestampType(), False)\n",
    "])\n",
    "\n",
    "print(\"Schema defined for inventory data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e43bc42-3aa7-4ffe-9841-f3d05e1a2eab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records processed: 20\nRecords with null inventory_id: 0\nRecords with negative stock: 0\nRecords with negative reserved: 0\nRecords with invalid available: 0\nValid records: 20\nInvalid records: 0\n"
     ]
    }
   ],
   "source": [
    "# Read and validate inventory data\n",
    "try:\n",
    "    # Read CSV files with schema validation\n",
    "    df_inventory = spark.read.schema(inventory_schema).csv(source_dir, header=True, dateFormat=\"yyyy-MM-dd\", timestampFormat=\"yyyy-MM-dd HH:mm:ss\")\n",
    "    \n",
    "    # Add processing metadata\n",
    "    df_inventory = df_inventory.withColumn(\"processed_timestamp\", F.current_timestamp()) \\\n",
    "                              .withColumn(\"batch_id\", F.lit(datetime.now().strftime(\"%Y%m%d_%H%M%S\"))) \\\n",
    "                              .withColumn(\"source_system\", F.lit(\"ecommerce_inventory\"))\n",
    "    \n",
    "    # Data quality checks\n",
    "    total_records = df_inventory.count()\n",
    "    null_inventory_ids = df_inventory.filter(F.col(\"inventory_id\").isNull()).count()\n",
    "    negative_stock = df_inventory.filter(F.col(\"stock_quantity\") < 0).count()\n",
    "    negative_reserved = df_inventory.filter(F.col(\"reserved_quantity\") < 0).count()\n",
    "    invalid_available = df_inventory.filter(F.col(\"available_quantity\") < 0).count()\n",
    "    \n",
    "    print(f\"Total records processed: {total_records}\")\n",
    "    print(f\"Records with null inventory_id: {null_inventory_ids}\")\n",
    "    print(f\"Records with negative stock: {negative_stock}\")\n",
    "    print(f\"Records with negative reserved: {negative_reserved}\")\n",
    "    print(f\"Records with invalid available: {invalid_available}\")\n",
    "    \n",
    "    # Filter out valid records - Fixed boolean logic\n",
    "    df_valid_inventory = df_inventory.filter(\n",
    "        (F.col(\"inventory_id\").isNotNull()) & \n",
    "        (F.col(\"stock_quantity\") >= 0) & \n",
    "        (F.col(\"reserved_quantity\") >= 0) & \n",
    "        (F.col(\"available_quantity\") >= 0)\n",
    "    )\n",
    "    \n",
    "    # Capture invalid records for error handling - Fixed boolean logic\n",
    "    df_invalid_inventory = df_inventory.filter(\n",
    "        (F.col(\"inventory_id\").isNull()) | \n",
    "        (F.col(\"stock_quantity\") < 0) | \n",
    "        (F.col(\"reserved_quantity\") < 0) | \n",
    "        (F.col(\"available_quantity\") < 0)\n",
    "    )\n",
    "    \n",
    "    valid_records = df_valid_inventory.count()\n",
    "    invalid_records = df_invalid_inventory.count()\n",
    "    \n",
    "    print(f\"Valid records: {valid_records}\")\n",
    "    print(f\"Invalid records: {invalid_records}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error reading inventory data: {str(e)}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a649a730-3509-48be-a2dd-3633167ed4b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data enrichment completed\n"
     ]
    }
   ],
   "source": [
    "# Data enrichment - Inventory analytics\n",
    "try:\n",
    "    # Calculate inventory turnover metrics\n",
    "    df_valid_inventory = df_valid_inventory.withColumn(\n",
    "        \"stock_utilization_rate\",\n",
    "        F.when(F.col(\"stock_quantity\") > 0, F.col(\"reserved_quantity\") / F.col(\"stock_quantity\"))\n",
    "         .otherwise(F.lit(0))\n",
    "    )\n",
    "    \n",
    "    # Create stock status categories\n",
    "    df_valid_inventory = df_valid_inventory.withColumn(\n",
    "        \"stock_status\",\n",
    "        F.when(F.col(\"available_quantity\") == 0, \"Out of Stock\")\n",
    "         .when(F.col(\"available_quantity\") <= F.col(\"reorder_level\"), \"Reorder Required\")\n",
    "         .when(F.col(\"available_quantity\") <= F.col(\"reorder_level\") * 2, \"Low Stock\")\n",
    "         .otherwise(\"In Stock\")\n",
    "    )\n",
    "\n",
    "\n",
    "    print(\"Data enrichment completed\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error in data enrichment: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcf2c95c-bfa6-4ea2-b65d-f613b040fa8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 20 valid inventory records to staging table\n"
     ]
    }
   ],
   "source": [
    "# Write valid data to staging table\n",
    "try:\n",
    "    # Create or overwrite staging table\n",
    "    df_valid_inventory.write.format(\"delta\").mode(\"overwrite\").saveAsTable(stage_table)\n",
    "    print(f\"Successfully loaded {valid_records} valid inventory records to staging table\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error writing to staging table: {str(e)}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30993cde-3c9b-4491-b08e-d2e1aa854b5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archived: inventory_2025_01_15.csv\nSuccessfully archived 1 files\n"
     ]
    }
   ],
   "source": [
    "# Archive processed files\n",
    "try:\n",
    "    # List all files in the source directory\n",
    "    files = dbutils.fs.ls(source_dir)\n",
    "    \n",
    "    archived_count = 0\n",
    "    for file in files:\n",
    "        if file.name.endswith('.csv'):\n",
    "            src_path = file.path\n",
    "            archive_path = archive_dir + file.name\n",
    "            \n",
    "            # Move the file to archive\n",
    "            dbutils.fs.mv(src_path, archive_path)\n",
    "            archived_count += 1\n",
    "            print(f\"Archived: {file.name}\")\n",
    "    \n",
    "    print(f\"Successfully archived {archived_count} files\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error archiving files: {str(e)}\")\n",
    "    raise\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_inventory_stage_load",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}